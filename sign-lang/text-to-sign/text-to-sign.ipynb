{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import cv2\n",
    "import PIL.Image, PIL.ImageTk\n",
    "import openai\n",
    "import json\n",
    "\n",
    "class VideoApp:\n",
    "    def __init__(self, root):\n",
    "\n",
    "        self.root = root\n",
    "        self.root.title(\"Video Player\")\n",
    "\n",
    "        # Entry field for video file path\n",
    "        self.entry_video_path = tk.Entry(self.root)\n",
    "        font = ('calibri', 12, 'normal')\n",
    "        self.entry_video_path.config(font=font)\n",
    "        self.entry_video_path.pack(padx=5, pady=5, fill=tk.X)\n",
    "\n",
    "        self.video_frame = tk.Frame(self.root)\n",
    "        self.video_frame.pack(padx=10, pady=10)\n",
    "\n",
    "        self.canvas = tk.Canvas(self.video_frame, width=640, height=480)\n",
    "        self.canvas.pack()\n",
    "\n",
    "        self.btn_open = tk.Button(self.root, text=\"Generate translation\", command=self.generate_video)\n",
    "        self.btn_open.pack(pady=5)\n",
    "\n",
    "        self.cap = None\n",
    "        self.paused = False\n",
    "        self.update()\n",
    "\n",
    "    def generate_video(self):\n",
    "        text_phrase = self.entry_video_path.get()\n",
    "\n",
    "        path_prefix = 'C:\\\\Users\\\\cioba\\\\Downloads\\\\archive\\\\videos\\\\'\n",
    "        # context = \"Your task is to break pharases into tokens based on sign language. Each token represents a combination of word/words to translate a phrases to respective ASL signs. I will give you the phrases. You're response format is going to be the tokens seperated by a comma, nothing more.\"\n",
    "        context = \"Your task is to take the given phrase, rewrite it so it respects the syntax and grammar of ASL (for example, a phrase like 'Hello, how are you?' should become 'hello how you',or that the verbs should be in infinitive form, or replacing 'I' with 'me'), and then output the words obtained, separated by spaces. Be sure to maintain the same order of the tokens as the words appear in the phrase. \"\n",
    "        #text_phrase = \"We watched a funny movie last night\"\n",
    "        openai.api_key = \"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\"\n",
    "\n",
    "        response = openai.completions.create(\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            prompt=context + \"\\n\" + text_phrase,\n",
    "            temperature=0,\n",
    "            max_tokens=60,\n",
    "        )\n",
    "\n",
    "        tokens = response.choices[0].text.strip().split(' ')\n",
    "        print(tokens)\n",
    "\n",
    "        with open('WLASL_v0.3.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        gloss = \"gloss\"\n",
    "        video_id = \"video_id\"\n",
    "        video_id_list = []\n",
    "        missing_id_list = []\n",
    "        missing_path = 'C:\\\\Users\\\\cioba\\\\Downloads\\\\archive\\\\missing.txt'\n",
    "\n",
    "        # we make a list of video ids that are missing\n",
    "        with open(missing_path, 'r') as f:\n",
    "            for line in f:\n",
    "                missing_id_list.append(line.strip())\n",
    "\n",
    "        print(missing_id_list)\n",
    "\n",
    "        print(tokens)\n",
    "        for token in tokens:\n",
    "            for item in data:\n",
    "                if item[gloss] == token:\n",
    "                    counter = 0\n",
    "                    # we need to search for the video if in the archive/missing.txt file\n",
    "                    # if it is not there, we can add it to the list\n",
    "                    # if it is there, we go to the next token\n",
    "                    while counter < len(item['instances']):\n",
    "                        if item['instances'][counter][video_id] not in missing_id_list:\n",
    "                            video_id_list.append(item['instances'][counter][video_id])\n",
    "                            break\n",
    "                        counter += 1\n",
    "                    if counter == len(item['instances']):\n",
    "                        print(\"Token: \" + token + \" is missing\")\n",
    "                    break\n",
    "            \n",
    "        print(video_id_list)\n",
    "\n",
    "\n",
    "        from moviepy.editor import VideoFileClip, concatenate_videoclips, vfx\n",
    "\n",
    "        clip_list = []\n",
    "\n",
    "        standard_frame_rate = 24  # Define your desired frame rate\n",
    "        standard_resolution = (1280, 720)\n",
    "        processed_videos = []\n",
    "\n",
    "        print(video_id_list)\n",
    "\n",
    "        for video_url in video_id_list:\n",
    "            video_path = path_prefix + video_url + \".mp4\"\n",
    "            clip = VideoFileClip(video_path)\n",
    "            clip = clip.resize(standard_resolution)\n",
    "            clip = clip.set_fps(standard_frame_rate)\n",
    "\n",
    "            clip_list.append(clip)\n",
    "            processed_videos.append(video_url)\n",
    "\n",
    "        final_clip = concatenate_videoclips(clip_list)\n",
    "        final_clip = final_clip.fx(vfx.speedx, 2)\n",
    "        \n",
    "        # Write the result to a file and only play it after it is done writing\n",
    "        final_clip.write_videofile(\"result.mp4\")\n",
    "        final_clip.close()\n",
    "\n",
    "        self.open_video(\"result.mp4\")\n",
    "\n",
    "    def open_video(self, file_path):\n",
    "        if file_path:\n",
    "            cap = cv2.VideoCapture(file_path)\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                cv2.imshow('Video', frame)\n",
    "                if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                    break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        #file_path = filedialog.askopenfilename()\n",
    "        #if file_path:\n",
    "        #   self.cap = cv2.VideoCapture(file_path)\n",
    "\n",
    "    def update(self):\n",
    "        if self.cap is not None and self.cap.isOpened():\n",
    "            ret, frame = self.cap.read()\n",
    "            if ret:\n",
    "                if not self.paused:\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    frame = PIL.Image.fromarray(frame)\n",
    "                    self.photo = PIL.ImageTk.PhotoImage(image=frame)\n",
    "                    self.canvas.create_image(0, 0, anchor=tk.NW, image=self.photo)\n",
    "        self.root.after(10, self.update)\n",
    "\n",
    "# Create the main application window\n",
    "root = tk.Tk()\n",
    "app = VideoApp(root)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
